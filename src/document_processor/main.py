"""
æä¾›å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµæ°´çº¿ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

from src.document_processor.processor import DocumentProcessor, create_processor
from src.document_processor.loader import BaseDocumentLoader

from langchain_classic.schema import Document

class BatchDocumentProcessor:
    """
    æ‰¹é‡æ–‡æ¡£å¤„ç†å™¨
    æ”¯æŒå¹¶å‘å¤„ç†å¤šä¸ªæ–‡æ¡£ï¼Œç”Ÿæˆå¤„ç†æŠ¥å‘Š
    """

    def __init__(self, 
                 processor_config: Optional[Dict[str, Any]] = None,
                 max_workers: int = 3,
                 output_dir: str = "data/processed"):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        
        Args:
            processor_config: æ–‡æ¡£å¤„ç†å™¨é…ç½®
            max_workers: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
            output_dir: è¾“å‡ºç›®å½•
        """
        self.processor_config = processor_config or {}
        self.max_workers = max_workers
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.processor = create_processor(**self.processor_config)
        self.logger = logging.getLogger("BatchProcessor")

    def process_single(
            self,
            file_path:str,
            save_to_file: bool = False
    ) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£å¹¶è¿”å›è¯¦ç»†ç»“æœ
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            save_to_file: æ˜¯å¦å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        result = {
            "file_path": file_path,
            "file_name": Path(file_path).name,
            "status": "pending",
            "start_time": datetime.now().isoformat(),
            "chunks": [],
            "error": None
        }

        try:
            chunks: List[Document] = self.processor.procecss_cocument(file_path=file_path)

            result.update({
                "status": "success",
                "end_time": datetime.now().isoformat(),
                "chunk_count": len(chunks),
                "chunks_preview": [
                    {
                        "content_preview": chunk.metadata.get("content_preview", ""),
                        "chunk_size": len(chunk.page_content),
                        "metadata_keys": list(chunk.metadata.keys())
                    } for chunk in chunks[:3]
                ]
            })

            if save_to_file:
                self._save_processing_result(file_path, chunks)
            
            self.logger.info(f"âœ… å¤„ç†å®Œæˆ: {file_path} ({len(chunks)} ä¸ªå—)")
        
        except Exception as e:
            result.update({
                'status': "error",
                "end_time": datetime.now().isoformat(),
                "error": str(e)
            })
            self.logger.error(f"âŒ å¤„ç†å¤±è´¥: {file_path} - {e}")
        
        return result
    

    def _save_processing_result(self, file_path:str, chunks: List[Document]):
        """ä¿å­˜å¤„ç†ç»“æœåˆ°Jsonæ–‡ä»¶"""
        file_name= Path(file_path).name
        output_file = self.output_dir / f"{file_name}_processed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        save_data = {
            "source_file": file_path,
            "processed_time": datetime.now().isoformat(),
            "chunk_count": len(chunks),
            "chunks": [
                {
                    "content": chunk.page_content,
                    "metadata": chunk.metadata
                } for chunk in chunks
            ]
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        self.logger.debug(f"å¤„ç†ç»“æœä¿å­˜åˆ°: {output_file}")

    def process_batch(
            self,
            file_paths: List[str],
            save_results: bool = True
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£
        
        Args:
            file_paths: æ–‡æ¡£è·¯å¾„åˆ—è¡¨
            save_results: æ˜¯å¦ä¿å­˜å¤„ç†ç»“æœ
            
        Returns:
            æ‰¹é‡å¤„ç†æŠ¥å‘Š
        """
        self.logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(file_paths)} ä¸ªæ–‡æ¡£")

        report = {
            "batch_id": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "start_time": datetime.now().isoformat(),
            "total_files": len(file_paths),
            "processed_files": 0,
            "successful": 0,
            "failed": 0,
            "results": [],
            "summary": {}
        }

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_file = {
                executor.submit(self.process_single, fp, save_results): fp for fp in file_paths
            }

            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    result = future.result()
                    report['results'].append(result)

                    if result['status'] == 'success':
                        report['successful'] += 1
                    else:
                        report['failed'] += 1
                
                except Exception as e:
                    error_result = {
                        "file_path": file_path,
                        "status": 'error',
                        "error": str(e)
                    }
                    report['results'].append(error_result)
                    report['failed'] += 1
        end_time = datetime.now().isoformat()
        report.update({
            "end_time": end_time,
            "processed_files": report["successful"] + report["failed"],
            "summary": {
                "success_rate": report["successful"] / len(file_paths) if len(file_paths) > 0 else 0,
                "average_chunks_per_file": self._calculate_average_chunks(report["results"]),
                "processing_time": self._calculate_processing_time(report, end_time)
            }
        })

        # ä¿å­˜æ‰¹å¤„ç†æŠ¥å‘Š
        self._save_batch_report(report)
        
        self.logger.info(
            f"æ‰¹é‡å¤„ç†å®Œæˆã€‚æˆåŠŸ: {report['successful']}, å¤±è´¥: {report['failed']}, "
            f"æˆåŠŸç‡: {report['summary']['success_rate']:.2%}"
        )
        
        return report
    
    def _calculate_average_chunks(self, results: List[Dict]) -> float:
        """è®¡ç®—å¹³å‡æ¯ä¸ªæ–‡ä»¶çš„å—æ•°é‡"""
        successful_results = [r for r in results if r.get('status') == 'success']
        if not successful_results:
            return 0
        
        total_chunks = sum(r.get("chunk_count", 0) for r in successful_results)
        return total_chunks / len(successful_results)
    
    def _calculate_processing_time(self, report: Dict, end_time:str) -> str:
        """è®¡ç®—å¤„ç†æ—¶é—´"""

        start= datetime.fromisoformat(report['start_time'])
        end =  datetime.fromisoformat(end_time)
        duration = end - start
        return str(duration)
    
    def _save_batch_report(self, report: Dict):
        """ä¿å­˜æ‰¹å¤„ç†æŠ¥å‘Š"""

        report_file = self.output_dir / f"batch_report_{report['batch_id']}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"æ‰¹å¤„ç†æŠ¥å‘Šä¿å­˜åˆ°: {report_file}")


def main():
    """ä¸»å‡½æ•°ï¼Œå‘½ä»¤è¡Œå…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description="æ™ºèƒ½æ–‡æ¡£å¤„ç†ç³»ç»Ÿ")
    parser.add_argument('--input', default="data/documents", help="è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„")
    parser.add_argument("--output", "-o", default="data/processed", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--chunk-size", type=int, default=1000, help="æ–‡æœ¬å—å¤§å°")
    parser.add_argument("--chunk-overlap", type=int, default=200, help="æ–‡æœ¬å—é‡å å¤§å°")
    parser.add_argument("--workers", '-w', type=int, default=3, help="å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°")
    parser.add_argument("--test", action='store_true', help="è¿è¡Œæµ‹è¯•æ¨¡å¼")

    args = parser.parse_args()

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    if args.test:
        run_tests()
        return
    
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {args.input}")
        return
    
    file_paths = []
    if input_path.is_file():
        file_paths = [str(input_path)]
    else:
        # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
        supported_extensions = ['.pdf', '.docx', '.doc', '.pptx', '.ppt']
        for ext in supported_extensions:
             file_paths.extend(list(input_path.glob(f"**/*{ext}")))
        file_paths = [str(fp) for fp in file_paths]

    if not file_paths:
        print(f"âŒ æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶: {args.input}")
        return
    
    print(f"ğŸ“‚ æ‰¾åˆ° {len(file_paths)} ä¸ªæ–‡æ¡£æ–‡ä»¶")

    # åˆ›å»ºæ‰¹é‡å¤„ç†å™¨
    processor_config = {
        "chunk_size": args.chunk_size,
        "chunk_overlap": args.chunk_overlap
    }

    batch_processor = BatchDocumentProcessor(
        processor_config=processor_config,
        max_workers=args.workers,
        output_dir=args.output
    )

    # å¼€å§‹å¤„ç†
    print("ğŸ”„ å¼€å§‹å¤„ç†æ–‡æ¡£...")
    report = batch_processor.process_batch(file_paths, save_results=True)
    
    # æ˜¾ç¤ºæ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š å¤„ç†æ€»ç»“")
    print("=" * 60)
    print(f"æ€»æ–‡ä»¶æ•°: {report['total_files']}")
    print(f"å¤„ç†æˆåŠŸ: {report['successful']}")
    print(f"å¤„ç†å¤±è´¥: {report['failed']}")
    print(f"æˆåŠŸç‡: {report['summary']['success_rate']:.2%}")
    print(f"å¹³å‡æ¯ä¸ªæ–‡ä»¶å—æ•°: {report['summary']['average_chunks_per_file']:.1f}")
    print(f"å¤„ç†æ—¶é—´: {report['summary']['processing_time']}")
    print(f"è¾“å‡ºç›®å½•: {args.output}")
    print("=" * 60)



# é›†æˆæµ‹è¯•å‡½æ•°

def run_tests():
    """è¿è¡Œé›†æˆæµ‹è¯•"""
    print("ğŸ§ª è¿è¡Œé›†æˆæµ‹è¯•...")

    test_dir = Path("data/test_integration")
    test_dir.mkdir(parents=True, exist_ok=True)

    test_files = []

    # åˆ›å»ºæµ‹è¯•æ–‡æœ¬æ–‡ä»¶å¹¶è½¬æ¢ä¸ºå…¶ä»–æ ¼å¼ï¼ˆæ¨¡æ‹Ÿï¼‰
    test_content = """è¿™æ˜¯ä¸€ä¸ªé›†æˆæµ‹è¯•æ–‡æ¡£ã€‚
    
        ç¬¬1ç« ï¼šæ–‡æ¡£å¤„ç†ç³»ç»Ÿ
        æœ¬ç³»ç»Ÿæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼ŒåŒ…æ‹¬PDFã€Wordå’ŒPPTã€‚

        ç¬¬2ç« ï¼šåŠŸèƒ½ç‰¹ç‚¹
        1. è‡ªåŠ¨æ–‡æœ¬åˆ†å‰²
        2. å…ƒæ•°æ®æå–
        3. æ‰¹é‡å¤„ç†
        4. è´¨é‡éªŒè¯

        ç¬¬3ç« ï¼šæ€§èƒ½æŒ‡æ ‡
        å¤„ç†é€Ÿåº¦ï¼šçº¦100é¡µ/åˆ†é’Ÿ
        å‡†ç¡®ç‡ï¼š99.5%ä»¥ä¸Š
        æ”¯æŒå¹¶å‘ï¼šæœ€å¤š10ä¸ªæ–‡æ¡£åŒæ—¶å¤„ç†
            
        è¿™æ˜¯ä¸€ä¸ªè¾ƒé•¿çš„æ®µè½ï¼Œç”¨äºæµ‹è¯•æ–‡æœ¬åˆ†å‰²å™¨æ˜¯å¦èƒ½æ­£ç¡®åœ°å°†é•¿æ–‡æœ¬åˆ†å‰²æˆé€‚å½“å¤§å°çš„å—ã€‚æ–‡æœ¬åˆ†å‰²æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œå®ƒç›´æ¥å½±å“åç»­çš„æ£€ç´¢å’Œç”Ÿæˆæ•ˆæœã€‚ä¸€ä¸ªå¥½çš„åˆ†å‰²ç­–ç•¥åº”è¯¥ä¿æŒè¯­ä¹‰çš„å®Œæ•´æ€§ï¼ŒåŒæ—¶æ§åˆ¶å—çš„å¤§å°ä»¥ä¾¿äºå¤„ç†ã€‚"""
    
    # åˆ›å»ºä¸åŒæ ¼å¼çš„æµ‹è¯•æ–‡ä»¶
    formats = [
        (".pdf", test_content),
        (".doc", f"# æµ‹è¯•æ–‡æ¡£\n\n{test_content}"),
    ]

    for ext, content in formats:
        test_file = test_dir / f"test_document{ext}"
        test_file.write_text(content, encoding='utf-8')
        test_files.append(str(test_file))

    print(f"ğŸ“ åˆ›å»ºäº† {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")

    print("\n1. æµ‹è¯•å•ä¸ªæ–‡ä»¶å¤„ç†:")

    processor = create_processor(chunk_size=500, chunk_overlap=100)

    for test_file in test_files[:1]:
        try:
            chunks = processor.procecss_cocument(test_file)
            print(f"   âœ… {Path(test_file).name}: {len(chunks)} ä¸ªæ–‡æœ¬å—")

            if chunks:
                print(f"      ç¤ºä¾‹å—å¤§å°: {len(chunks[0].page_content)} å­—ç¬¦")
                print(f"      å…ƒæ•°æ®å­—æ®µ: {list(chunks[0].metadata.keys())}")
        except Exception as e:
            print(f"   âŒ {Path(test_file).name}: å¤±è´¥ - {e}")
    

    print("\n2. æµ‹è¯•æ‰¹é‡å¤„ç†:")

    batch_processor = BatchDocumentProcessor(
        max_workers=2, output_dir='data/test_output'
    )

    report = batch_processor.process_batch(test_files, save_results=False)

    print(f"   æ€»æ–‡ä»¶æ•°: {report['total_files']}")
    print(f"   æˆåŠŸ: {report['successful']}")
    print(f"   å¤±è´¥: {report['failed']}")
    print(f"   æˆåŠŸç‡: {report['summary']['success_rate']:.2%}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    print("\n3. æ¸…ç†æµ‹è¯•æ–‡ä»¶...")
    for test_file in test_dir.glob("*"):
        test_file.unlink()
    test_dir.rmdir()
    
    print("âœ… é›†æˆæµ‹è¯•å®Œæˆ")


# ============ 4. æ¨¡å—å¯¼å…¥æ¥å£ ============
def get_document_processor(config: Optional[Dict] = None) -> DocumentProcessor:
    """è·å–æ–‡æ¡£å¤„ç†å™¨å®ä¾‹ï¼ˆä¾›å…¶ä»–æ¨¡å—å¯¼å…¥ï¼‰"""
    return create_processor(**(config or {}))


def get_batch_processor(
    max_workers: int = 3,
    output_dir: str = "data/processed"
) -> BatchDocumentProcessor:
    """è·å–æ‰¹é‡å¤„ç†å™¨å®ä¾‹ï¼ˆä¾›å…¶ä»–æ¨¡å—å¯¼å…¥ï¼‰"""
    return BatchDocumentProcessor(
        max_workers=max_workers,
        output_dir=output_dir
    )


if __name__ == "__main__":
    main()