"""
é›†æˆæ–‡æ¡£åŠ è½½ã€åˆ†å—ã€å…ƒæ•°æ®æå–ç­‰åŠŸèƒ½
"""

import os
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from pathlib import Path

from langchain_classic.schema import Document
from langchain_classic.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, MarkdownTextSplitter


from .loader import BaseDocumentLoader, get_text_splitter


@dataclass
class ProcessingConfig:
    """æ–‡æ¡£å¤„ç†é…ç½®"""
    chunk_size: int = 1000
    chunk_overlap: int = 200
    splitter_type: str = 'recursive' # recursive, character, markdown
    extract_metadata: bool = True
    min_chunk_size: int = 50
    max_chunk_size: int = 2000



class DocumentProcessor:
    """
    æ–‡æ¡£å¤„ç†å™¨ï¼šè´Ÿè´£æ–‡æ¡£åŠ è½½ã€åˆ†å‰²å’Œé¢„å¤„ç†çš„å…¨æµç¨‹
    
    åŠŸèƒ½ï¼š
    1. å¤šæ ¼å¼æ–‡æ¡£åŠ è½½ (PDF, Word, PPTX)
    2. å¤šç§åˆ†å—ç­–ç•¥ (é€’å½’å­—ç¬¦ã€å­—ç¬¦ã€Markdown)
    3. å…ƒæ•°æ®æå–å’Œå¢å¼º
    4. åˆ†å—è´¨é‡éªŒè¯
    """

    def __init__(self,
               config: Optional[ProcessingConfig]= None, 
               logger: Optional[logging.Logger]= None):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        
        Args:
            config: å¤„ç†é…ç½®ï¼Œå¦‚æœªæä¾›åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            logger: æ—¥å¿—è®°å½•å™¨ï¼Œå¦‚æœªæä¾›åˆ™åˆ›å»ºæ–°çš„
        """
        self.config = config or ProcessingConfig()
        self.logger = logger or self._setup_logger()
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger("DocumentProcessor")
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _get_splitter(self) -> Any:
        """æ ¹æ®é…ç½®è·å–æ–‡æœ¬åˆ†å‰²å™¨"""
        if self.config.splitter_type == 'character':
            return CharacterTextSplitter(
                chunk_size = self.config.chunk_size,
                chunk_overlap = self.config.chunk_overlap,
                separator='\n'
            )
        elif self.config.splitter_type == 'markdown':
            return MarkdownTextSplitter(
                chunk_size = self.config.chunk_size,
                chunk_overlap = self.config.chunk_overlap,
            )
        else:
            return get_text_splitter(
                chunk_size = self.config.chunk_size,
                chunk_overlap = self.config.chunk_overlap,
            )
        
    def _enhance_metadata(self, document: Document, file_path: str, chunk_index:int) -> Dict[str, Any]:
        """
        å¢å¼ºæ–‡æ¡£å¿«çš„å…ƒæ•°æ®
        ä¸ºæ¯ä¸ªæ–‡æœ¬å—æ·»åŠ ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä½¿åç»­çš„æ£€ç´¢ã€åˆ†æå’Œå±•ç¤ºæ›´åŠ æ™ºèƒ½ã€‚
        """
        metadata = document.metadata.copy() if document.metadata else {}
        # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
        metadata.update({
            "source_file": file_path,
            "file_name": Path(file_path).name,
            "file_type": Path(file_path).suffix.lower(),
            "chunk_index": chunk_index,
            "chunk_size": len(document.page_content),
            "processor": "DocumentProcessor_v1.0",
        })

        # æå–å‰å‡ ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
        content_preview = document.page_content[:100].replace("\n", " ")
        metadata["content_preview"] = content_preview

        return metadata
    
    def _validate_chunk(self, chunk:Document) -> Tuple[bool, str]:
        """éªŒè¯æ–‡æœ¬å—è´¨é‡"""
        content = chunk.page_content.strip()

        # æ£€æŸ¥å†…å®¹
        if not content:
            return False, 'ç©ºå†…å®¹'
        
        if len(content) < self.config.min_chunk_size:
            return False, 'å†…å®¹è¿‡çŸ­'
        
        if len(content) > self.config.max_chunk_size:
            return False, 'å†…å®¹è¿‡å¤š'
        
        return True, 'æœ‰æ•ˆ'
    
    def procecss_cocument(
            self,
            file_path: str,
            return_raw: bool = False
    ) -> List[Document]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£çš„å…¨æµç¨‹"""

        self.logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£ {file_path}")

        try:
            loader = BaseDocumentLoader.get_loader(file_path)
            raw_documents = loader.load()
            self.logger.info(f"æ–‡æ¡£åŠ è½½æˆåŠŸï¼Œ å…± {len(raw_documents)} ä¸ªåŸå§‹é¡µé¢/éƒ¨åˆ†")

            splitter = self._get_splitter()
            chunks = splitter.split_documents(raw_documents)
            self.logger.info(f"æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œ å…±{len(chunks)}ä¸ªæ–‡æœ¬å—")

            if return_raw:
                return chunks
            
            # å¤„ç†æ¯ä¸ªæ–‡æœ¬å—ï¼ŒéªŒè¯ + å…ƒæ•°æ®å¢å¼º
            processed_chunks = []
            valid_count = 0
            invalid_count = 0

            for i, chunk in enumerate(chunks):
                # éªŒè¯æ–‡æœ¬å—çš„è´¨é‡
                is_valid, reason = self._validate_chunk(chunk)
                if is_valid:
                    # å¢å¼ºå…ƒæ•°æ®
                    enhanced_metadata = self._enhance_metadata(chunk, file_path, i)
                    processed_chunk = Document(page_content=chunk.page_content)
                    processed_chunk.metadata = enhanced_metadata
                    processed_chunks.append(processed_chunk)
                    valid_count += 1
                else:
                    self.logger.info(f"æ–‡æœ¬å—è¢«è¿‡æ»¤ï¼ŒåŸå› æ˜¯ {reason}")
                    invalid_count += 1
            
            self.logger.info(
                f"æ–‡æ¡£å¤„ç†å®Œæ¯•ï¼Œæœ‰æ•ˆæ–‡æœ¬å—çš„æ•°é‡ï¼š {valid_count}, æ— æ•ˆæ–‡æœ¬å—ï¼š {invalid_count}"
            )
            return processed_chunks
        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡æ¡£çš„æ—¶å€™å‡ºé”™ï¼š {e}")
            raise

    
    def compare_split_strategies(self, file_path:str, strategies: List[str] = None) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸åŒåˆ†å—ç­–ç•¥çš„æ•ˆæœ"""

        if strategies is None:
            strategies = ["recursive", "character", "markdown"]
        
        results = {}
        
        for strategy in strategies:
            try:
                original_strategy = self.config.splitter_type
                self.config.splitter_type = strategy

                chunks = self.procecss_cocument(file_path=file_path, return_raw= True)

                total_chunks = len(chunks)
                avg_length = sum(len(c.page_content) for c in chunks) / total_chunks if total_chunks > 0 else 0
                max_length = max((len(c.page_content) for c in chunks), default=0)
                min_length = min((len(c.page_content) for c in chunks), default=0)

                results[strategy] = {
                    "chunk_count": total_chunks,
                    'avg_chunk_size': round(avg_length, 2),
                    'max_chunk_size': max_length,
                    'min_chunk_size': min_length,
                    'sample_chunks': [
                        c.page_content[:100] + "..." for c in chunks[:2]
                    ] if chunks else []
                }

                self.config.splitter_type = original_strategy
            except Exception as e:
                results[strategy] = {'error': str(e)}
        
        return results
    


def create_processor(
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        splitter_type: str = 'recursive'
) -> DocumentProcessor:
    """å·¥å‚å‡½æ•°ï¼Œåˆ›å»ºé…ç½®å¥½çš„æ–‡æ¡£å¤„ç†å™¨"""
    config = ProcessingConfig(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        splitter_type=splitter_type
    )
    return DocumentProcessor(config=config)

    
    
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_file = "data/documents/test.pdf"
    test_file_path = Path(test_file)

    if test_file_path.exists():
        try:
            print("=" * 60)
            print("ğŸ“„ æµ‹è¯•æ–‡æ¡£å¤„ç†å™¨")
            print("=" * 60)

            processor = create_processor()
            print('æ–‡æ¡£å¤„ç†å™¨åˆ›å»ºæˆåŠŸ')

            print(f"\n 1. å¤„ç†æ–‡æ¡£ {test_file}")
            processed_chunks = processor.procecss_cocument(test_file)
            print(f"å¤„ç†å®Œæˆ å¾—åˆ° {len(processed_chunks)} ä¸ªæœ‰æ•ˆæ–‡æœ¬å—")


            if processed_chunks:
                # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡æœ¬å—çš„å…ƒæ•°æ®
                first_chunk = processed_chunks[0]
                print(f"\n 2. ç¬¬ä¸€ä¸ªæ–‡æœ¬å—çš„å…ƒæ•°æ®")
                for key, value in first_chunk.metadata.items():
                    print(f"{key}: {value}")
                
                # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                preview = first_chunk.page_content[:150] + "..." if len(first_chunk.page_content) > 150 else first_chunk.page_content
                print(f"\n 3. å†…å®¹é¢„è§ˆï¼š\n {preview}")
            
            # æµ‹è¯•æ¯”è¾ƒä¸åŒçš„åˆ†å—ç­–ç•¥
            print(f"\n æ¯”è¾ƒä¸åŒçš„åˆ†å—ç­–ç•¥")
            strategies_results = processor.compare_split_strategies(test_file)
            for strategy, result in strategies_results.items():
                print(f'\n {strategy.upper()} ç­–ç•¥')
                if 'error' in result:
                    print(f"é”™è¯¯: {result['error']}")
                else:
                    print(f"     æ–‡æœ¬å—æ•°é‡: {result['chunk_count']}")
                    print(f"     å¹³å‡å¤§å°: {result['avg_chunk_size']} å­—ç¬¦")
                    print(f"     æœ€å¤§å¤§å°: {result['max_chunk_size']} å­—ç¬¦")
                    print(f"     æœ€å°å¤§å°: {result['min_chunk_size']} å­—ç¬¦")
            
            print("\n" + "=" * 60)
            print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
            print("=" * 60)
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    else:
        print(f"âš ï¸ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        print("è¯·ç¡®ä¿å·²å°†æµ‹è¯•æ–‡æ¡£æ”¾å…¥ data/documents/ ç›®å½•")






